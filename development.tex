\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[hidelinks,colorlinks=false,urlcolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage[section]{placeins}

\title{Snp HeRitability Estimation Kit (SHREK) Development Log}
\date{\today}
\author{Sam Shing Wan, Choi\\ 
	\texttt{choishingwan@gmail.com}
	\and
	Johnny Sheung Him, Kwan\\
	\texttt{shkwan@hku.hk}
	\and
	Henry Chi-Ming, Leung\\
	\texttt{cmleung2@cs.hku.hk}
}


\begin{document}
	
	\maketitle
	\tableofcontents
	\newpage
	\section{Assumption in implementation}
	To make life easier, I made a number of assumption when I implement the programme and they are listed as follow
	\begin{enumerate}
		\item The intervals within each individual bed files are non-overlapping
		\item SNPs filtered out when reading the p-value file will not be included in the output no matter what (avoid storing unnecessary information)
		\item We will remove \emph{all} SNPs that are in perfect LD with each-other
		\begin{enumerate}
			\item Realistically, we can not remove \emph{all} SNPs that are in perfect LD with each-other without first computing the LD matrix for the whole genome. 
			However, that will incur a large speed penalty to the algorithm.
			To compromise, we remove SNPs that are in perfect LD with each-other \emph{within the same window}.
			Take into consideration of the calculation of variance, we will need to plan forward and consider SNPs within 4 windows. 
			That's definitely overkill but that avoid problem of crazy perfect LD interactions (to be honest, I would consider those as error, yet it is easier for me to just remove them)
		\end{enumerate}
		\item We do not consider a SNP ambiguous unless we need to perform flipping
		\item Only when the direction of effect is presented will we perform the detail variance estimation calculation.
		The equation for the variance calculation is:
		\begin{math}
			\boldsymbol{R_{sq}}^{-1}\boldsymbol{n}(4\boldsymbol{R}\circ(\boldsymbol{zz}^t)-2\boldsymbol{R_{sq}}^{-1})\boldsymbol{n}\boldsymbol{R_{sq}}^{-1}
		\end{math}
		where $\boldsymbol{n}$ is a square diagonal matrix with diagonal as the sample size and $\boldsymbol{z}$ is the vector of z-statistics.
	\end{enumerate}
	
	\section{To Do}
	\begin{enumerate}
		\item Compare the condition number of the LD matrix before and after the LD correction
		\item Compare the performance of case control estimation with different sample size 
		\item For LD correction, might also worth testing how the density and total number of SNPs affect the estimation
		\item For effect size estimation, don't use the complicated method
		\item Compare the speed of the new and old implementation (remember that the complex variance calculation do takes more time to perform)
	\end{enumerate}
	
	
	
	\section{January 25, 2016}
	We are going to re-write SHREK starting from today. 
	There are a number of main goal in this re-work
	\begin{enumerate}
		\item The whole parameter parsing 
		\item Better help messages
		\item Allow filtering by imputation score (0-1)
		\item Filtering of SNPs that have different reference/alternative allele with reference
		\item Use Armadillo instead of EIGEN to improve speed
		\item Filter \emph{all} SNPs that are in perfect LD
		\item Implement the advance variance calculation 
		\item Start documenting the codes
	\end{enumerate}
	\subsection{Finished Today}
	\begin{enumerate}
		\item Finished the error message for different modes
		\item Finished the parsing for binary trait
		\item Finished the parsing for quantitative trait
		\item Disabled risk prediction, should focus on the heritability estimation first
	\end{enumerate}
	\emph{I have not perform debugging, there can be error}
	\section{January 26, 2016}
	Continue on where I left yesterday
	Assumptions we made in the programme
	\begin{enumerate}
		\item p-value file has header
		\item the numbers return from Command class are index (0 based)
	\end{enumerate}
	I really want to write a more compacted code.
	Will try to restructure the command class
	\subsection{Finished Today}
	\begin{enumerate}
		\item Finished refining the usage messages
		\item Finished the basic parameter parsing (Still haven't finish the index based parameter though)
	\end{enumerate}
	\section{January 27, 2016}
	\subsection{Aim}
	We want to at least finish the region parsing and SNP class update today
	\subsection{Finished Today}
	\begin{enumerate}
		\item Completed the update of the Command class (compiled without problem)
		\item Completed the update of the Region class (compiled without problem)
		\item Completed the update of the Snp class (compiled without problem)
	\end{enumerate}
	I also \emph{haven't} test run the programme, so there might be additional syntax / logic error that are not picked up at the current stage.
	
	\section{January 28, 2016}
	\subsection{Finished Today}
	\begin{enumerate}
		\item Finished the checking of reference panel
		\item Found a different algorithm for popcount, which should be platform independent
		\item Installed armadillo and OpenBLAS
		\begin{enumerate}
			\item I have to add the OpenBLAS to the PATH and LD\_LIBRARY\_PATH for armadillo to detect it
		\end{enumerate}
		\item Updated the makefile accordingly
		\begin{enumerate}
			\item However, I have not updated the decomposition class, therefore there will be problem when we try to compile the programme
		\end{enumerate}
		\item Updated the $r^2$ calculation function such that it is using the correct sample size
		\begin{enumerate}
			\item The sample size when calculating the $r^2$ is defined as the number of samples containing \emph{both} SNPs
			\item Because of how complicated the genotype class is, I really don't want to modify it
			\item The main bottleneck for the computation is in the decomposition anyway, so it should be fine
		\end{enumerate}
		
	\end{enumerate}
	
	\section{January 29, 2016}
	Need to at least finish the getSNP function.
	Must be careful with it as this is not only complicated, but also extremely important for the whole programme.
	
	I spent whole day in annotating the document of the class to make sure I don't overlook any problem.
	
	\section{February 1, 2016}
	Start to implement the getSNP function. 
	A number of point to note
	\begin{enumerate}
		\item First get all the SNPs that passed all filtering (e.g. MAF)
		\item Then for each window, calculate the LD
		\item When encountered with perfect LD, we retain the \emph{first} SNP
		\begin{enumerate}
			\item The summary statistics will be the mean of all SNPs in perfect LD
			\item All SNPs except the first are \emph{removed} 
			\item So they will not appear in subsequent analysis
		\end{enumerate}
	\end{enumerate}
	
	\section{February 2, 2016}
	Try to perform extensive check on codes that has been written
	\subsection{Finished Today}
	\begin{enumerate}
		\item Finished debugging, everything up until the initialization of genotypeFileHandler is ok
	\end{enumerate}
	\section{February 3, 2016}
	Sat in a meeting with Pak today, now we have a few things to-do.
	
	\subsection{Finished Today}
	We have switched to use list instead of deque. 
	The benefit of list is that it is a double linked list, therefore it is much easier for one to remove the middle part.
	Such property is ideal for the handling of perfect LD. 
	However, it also means that we cannot access the elements of the list by index. 
	The only way for us to access the elements is to use the iterators and the advance function from the iterator library.
	Although this makes the implementation more difficult, it should be able to help us to speed up our code even if we want to perform perfect LD removal.
	\begin{enumerate}
		\item Extensive testing of the previous classes
		\item Now use iterators for the boundary, this is more dynamic and can work with the perfect LD removal without inducing much complication into the algorithm (however, the implementation itself can be complicated)
	\end{enumerate}
	\subsection{Problem identified today}
	So assuming that the SNPs A B C are occurring in the genome by alphabetical order, when we calculate the LD of A with B C, we do not observe any perfect LD. 
	This will lead us to continue with the calculation of SNP B and C. 
	However, if SNP B and SNP C is in perfect LD, we will need to go back to the LDs of SNP A to remove the column of SNP C.
	Not only is this procedure complicates the implementation, it will also significantly slow down the computation because of the constant resizing of the matrix.
	
	To conclude, it might be better for us to use the original ``sausage'' approach, where we should also locate all the perfect LD location.
	Then we can remove the perfect LD afterwards.
	
	\section{February 4, 2016}
	Was spending the whole day in implementing the ``sausage'' approach.
	However, it was found that the calculated $R^2$ are terribly wrong. 
	So now converting back to the old method of inclusion for the genotype reading.
	We will then check the implementation of the $R^2$ computation right after we have make sure the genotype reading is correct.
	\section{February 5, 2016}
	Try to continue on what we have been doing yesterday.
	\subsection{Finished Today}
	\begin{enumerate}
		\item 	Finished debugging the genotype reading.
				By using the old inclusion method, we avoid a lot of complication and the programme now works accordingly.
				We also checked the bit arithmetic and decided to use our original calculation which unlike what I previously thought, isn't platform specific.
				
	\end{enumerate}
	\subsection{Special Note}
	The NumberOfBit function in the usefulTools doesn't work in our case because our number is longer than uint32\_t. 
	I am rather uncertain how should we change the script accordingly because it involves extreme high level bit arithmetic.
	However, based on online sources, it seems like the \_\_builtin\_popcountll function will work as long as we compile the programme using gcc.
	So we will stick to that function for now.
	
	\section{February 11, 2016}
	Back from holiday, now taking the perfect LD seriously and start implementing the perfect LD removal algorithm.
	It is important to note that it is possible for the boundaries to be in perfect LD, therefore potentially changing the boundary.
	However, take into account of our current procedure, most of the time, only the last block need our attention, unless this is the beginning of the chromosome or decomposition region.
	\subsection{Finished Today}
	\begin{enumerate}
		\item Implemented the slow but simple version of the perfect LD removal, need to test run it.
		\item Solved some bugs, e.g. the genotype collection and LD construction bug (use the wrong index)
		\item Found that there are still problems with the perfect LD removal.
		Specifically, when the SNP at the bound is in perfect LD with previous SNPs, the handling are still problematic. 
		Need to change my test case in which I reduce the gap between the two SNPs (e.g. the 4$->$7)
	\end{enumerate}
	\section{February 12, 2016}
	Finishing the first part of debugging. 
	Basically, the LD construction seems to be ok for now.
	Also, I reused my old code for getting the mean of the test-statistics.
	Now, we store only the z-score in the Snp object.
	This reduces the number of information we store in Snp object but slightly increases the number of computation we have to perform.
	\subsection{Finished Today}
	\begin{enumerate}
		\item Finished all the test run for linkage construction (have not test the cross chromosome yet)
		\item Implemented the decomposition and variance calculation. (Using Tim's Equation)
		Will need to perform the simulation to see if the variance estimation works. 
		\item Updated the Snp class to accommodate the perfect LD computation.
	\end{enumerate}
	
	\subsection{Note}
	Unfortunately, I am not used to template usage in c++, therefore I will have to write two separate function for the decomposition of the matrix depending on whether if the sign is provided.
	
	An important finding regarding the linking of the library.
	You must have the armadillo/usr/lib folder under your LD\_LIBRARY\_PATH.
	Also, you will need -larmadillo when you compile and it should come \emph{after} the cpp file.
	
	When performing the test code, it was found that the pinv function of armadillo is actually 100 times faster than my self-implemented pseudo inverse. 
	Therefore, we should use their implementation instead.
	
	What we have to do next is to store the results of the estimated variance and to generate the required output.
	\section {February 15, 2016}
	Start working on the variance capturing for the complex variance calculation.
	The most difficult part is to determine the denominator of the calculation.
	The best way might be to use median, which is less sensitive to outliers.
	However, the problem is, how to calculate the median without storing the whole matrix, which is memory intensive?
	
	According to Tim, we only need the \emph{rows} of the target SNPs, therefore we only need to capture each \emph{rectangle} per run.
	Currently trying to list out all possible scenarios to avoid bugs.
	
	\section{February 16, 2016}
	Continue on what I was doing yesterday.
	\subsection{Finished Today}
	\begin{enumerate}
		\item Finished implementing the whole programme, now need to start the debugging hell
	\end{enumerate}
	\section{February 17, 2016}
	DEPENDENCY HELL!!!
	AND BUGS, A LOT OF BUGS!!!
	\subsection{Today's work}
	\begin{enumerate}
		\item Trying to debug the programme
		\item Found that gdb doesn't work (too old)
		\item Try to compile the gdb
		\item Failed, so tried to compile latest gcc and see if that works
		\item Need to make sure we recompile everything with the new gcc (otherwise, you will have tones of undefined referencing)
		\item The variance is most likely to be wrong
	\end{enumerate}
	\section{February 18, 2016}
	Early simulation results suggested that the MSE of the new programme doubles that of the old one which is really bad. 
	So now we need to check what's the problem of that.
	
	Using the last simulated data, it was found that by not performing the LD correction, we can get a similar result as the new implementation. 
	Therefore it is likely that the LD correction is accounting for the problem.
	
	\subsection{Note}
	Given the following plink simulation input, we would like to have an empirical variance of around 0.002729694
	
	
	\begin{tabular}{r}
		10 snp  0.05 0.05 0.05 0.05 1 0.001 0\\
		100 snp2 0.05 0.05 0.05 0.05 0.7 0.001 0\\
		890 null 0.05 0.05 0.05 0.05 0.8 0 0\\
	\end{tabular}
	
	\subsection{Finished Today}
	\begin{enumerate}
		\item The variance calculation has been fixed
		\item It seems like the programme can produce sensible results for now
	\end{enumerate}
	\section{February 19,2016}
	The use of List is very smart in a way that it preserves the iterator, however, using the profiling, it was found that the List operations are taking up most time. 
	Therefore, I will now have to change all the List usage back to deque, hopefully make things a bit faster
	
	\section{February 22, 2016}
	We have finished some basic debugging in the past few days.
	The heritability estimation now work as planned.
	However, the variance is still rather problematic.
	The original equation:
	\begin{equation}
	\mathrm{Var}(h^2)=\boldsymbol{R_{sq}}^{-1}\frac{4\boldsymbol{R}\circ\boldsymbol{zz}^t-2\boldsymbol{R_{sq}}}{n^2}\boldsymbol{R_{sq}}^{-1}
	\end{equation}
	Seems to return negative results. 
	We need to keep working on it.
	
	\begin{align}
	\mathrm{Var}(H) &= \boldsymbol{1^tR_{sq}^{-1}}\frac{4\boldsymbol{R}\circ\boldsymbol{\mu\mu^t}+2\boldsymbol{R_{sq}}}{n^2}\boldsymbol{R_{sq}^{-1}1}\nonumber\\
	&=\boldsymbol{1^tR_{sq}^{-1}}\frac{4\boldsymbol{R}\circ(\boldsymbol{zz^t-R})+2\boldsymbol{R_{sq}}}{n^2}\boldsymbol{R_{sq}^{-1}1} \nonumber\\
	&=\boldsymbol{R_{sq}}^{-1}\frac{4\boldsymbol{R}\circ\boldsymbol{zz}^t-2\boldsymbol{R_{sq}}}{n^2}\boldsymbol{R_{sq}}^{-1}
	\end{align}
	\section{February 23, 2016}
	The main problem with our simulation is the speed required. 
	From what we saw, the main bottle neck for the simulation is hapgen.
	So we might have to figure out a way to make it faster.
	
	\section{February 24, 2016}
	We are trying to tackle the problem of variance estimation.
	Seems like only when we use the sample genotypes can we correctly estimate the empirical variance.
	\section{February 25, 2016}
	Still trying to figure out the problem of the variance estimation. 
	One thing we found is that our current simulation method actually violates the assumption of fixed X in Tim's formular of phenotype simulation.
	Therefore, we now try to use a large reference panel (e.g. 10000+) samples to calculate the $\mathrm{Var}(X\beta)$ and use this for the calculation of phenotype.
	\newpage
	\section{Useful Resources}
	\begin{table}[t]
		\centering
		\begin{tabular}{rr}
			\toprule
			Description & URL \\
			\midrule
			Timing the functions in c++ programme & \href{http://stackoverflow.com/a/21995693}{link}\\
			\bottomrule
		\end{tabular}
	\end{table}
\end{document}